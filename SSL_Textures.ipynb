{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Material and Texture Representation Learning for Remote Sensing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <A54E39C4-6B62-3303-9BE6-7DB88EB078BF> /opt/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Standard packages\n",
    "import csv\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Math packages\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "# PyMca packages\n",
    "from PyMca5.PyMca import ConfigDict\n",
    "\n",
    "# XRF image processing packages\n",
    "import xrfip as xp\n",
    "import xrfip.optimization as opt\n",
    "from xrfip.optimization.losses import *\n",
    "\n",
    "from xrfip.utils.errors import *\n",
    "from xrfip.utils.units import *\n",
    "from xrfip.utils.visualizations import *\n",
    "from xrfip.utils.save_mask import save_mask\n",
    "\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "import pystac\n",
    "from pystac.client import Client\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet-34 Model\n",
    "class ResNet(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, channels):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.num_clusters = 64\n",
    "        self.in_conv = torch.nn.Conv2d(in_channels = channels, out_channels = 64, kernel_size = 7, padding = 3)\n",
    "        self.layers = make_layers(layer = ResBlock, num_layers = 4, in_channels = [64,128,256,512], out_channels = [128,256,512,channels], kernel_size = 3, padding = 1)\n",
    "        self.out_conv = torch.nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size= 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu =nn.ReLU(inplace = True)\n",
    "        self.softmax = nn.Softmax2d()\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.softplus = nn.Softplus()\n",
    "    def make_layers(layer, num_layers,inplanes, planes, strides,  kernel_size, padding):\n",
    "        strides = [strides] + [1] * (num_layers-1)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            stride = strides[i]\n",
    "            layers.append(ResBlock(inplanes), planes, stride)\n",
    "            inplanes = planes * i\n",
    "    def forward(self, x):\n",
    "        x = self.in_conv(x)\n",
    "        x = self.bn1(x)  \n",
    "        x = self.relu(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.softplus(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "            \n",
    "#Resblock is done, which operates as one block of ResNet with adding x and f(x)\n",
    "class ResBlock(nn.Module):\n",
    "    #we use expansion here to generate the array of expansion\n",
    "    expansion = 1\n",
    "    #initalization\n",
    "    def __init__(self, in_planes, planes, stride = 1, is_last = False):\n",
    "        super(ResBlock, self).init__()  \n",
    "        #standard convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        #stride != 1, that means we can take the shortcut because we are not at an inital block\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = torch.nn.functional(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(out)\n",
    "        pre = out\n",
    "        out = nn.ReLU(out)\n",
    "        if self.is_last:\n",
    "            return out, pre\n",
    "        else:\n",
    "            pre\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 11 (949958565.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    class TeRn(nn.Module):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 11\n"
     ]
    }
   ],
   "source": [
    "class LowLevelBlock(nn.Module):\n",
    "    def __init__(self, in_channels =3 , out_channels= 64):\n",
    "        super(LowLevelBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels =in_channels, out_channels= out_channels, kernel_size= 3, stride = 1, padding = 1)\n",
    "        self.act1 = nn.LeakyReLU(0.1, inplace=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels= out_channels, out_channels = out_channels, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.act2 = nn.LeakyReLU(0.1, inplace = False)\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.act2(self.conv2(x))\n",
    "        \n",
    "class RefineBlock(nn.Module):\n",
    "    def __init__(self, channels = 64):\n",
    "        super(RefineBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = channels, out_channels= channels, kernel_size=3, stride = 1, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels = channels, out_channels= channels, kernel_size= 3, stride = 1, padding = 1)\n",
    "        self.bn2 =nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "    def forward(self, x):\n",
    "        res = x \n",
    "        out = self.relu(self.BatchNorm2d(x))\n",
    "        \n",
    "#Texture Refinement Network (TeRn)\n",
    "class EvalNet(nn.Module):\n",
    "    \"\"\"φ_e: takes a token grid S (with masks) and predicts a keep/mask probability map.\"\"\"\n",
    "    def __init__(self, dimensions, n_heads, depth):\n",
    "        super(EvalNet, self).__init__()\n",
    "        layers += [\n",
    "            nn.MultiheadAttention(dimensions, n_heads, batch_first= True),\n",
    "            nn.LayerNorm(dimensions),\n",
    "            nn.Linear(dimensions),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dimensions)\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.to_logits = nn.Linear(dim = 1) #uses one logit per token\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        x = self.net(tokens)\n",
    "        logits = self.to_logits(x)\n",
    "        token = torch.sigmoid(logits)\n",
    "        return token\n",
    "\n",
    "class RefineNet(nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super(RefineNet, self).__init__()\n",
    "        layers += [\n",
    "            nn.MultiheadAttention(dim, n_heads, batch_first = True),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(dim)\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.to_logits = nn.Linear(dim =  1)\n",
    "    def forward(self, token):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .alias_multinomial import AliasMultinomial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class TextureEncoder(nn.Module):\n",
    "    def __init__(self, channels, num_clusters):\n",
    "        super(TextureEncoder, self).__init__()\n",
    "        self.inchannels, self.num_clusters = channels, num_clusters\n",
    "        std = 1. / ((num_clusters * channels)**0.5)\n",
    "        self.clusters = torch.zeros(num_clusters, channels, dtype = torch.float, requires_grad = True).uniform(-std, std)\n",
    "        self.scale =torch.zeros(num_clusters, dtype = torch.float, requires_grad= True).uniform_(-1,0)\n",
    "        \n",
    "    def l2_norm(self, x, clusters, scale) ->torch.tensor:\n",
    "        \"\"\" using l2 distance we find the weights of cluster centers which is needed for such feature extraction\n",
    "        args:\n",
    "        x: our input features\n",
    "        clsuters; learning clusters\n",
    "        scale: we scale by some term\n",
    "        \"\"\"\n",
    "        num_clusters, in_channels = clusters.size()\n",
    "        batch_size = x.size(0)\n",
    "        scale = scale.view((1,1, clusters)) \n",
    "        \n",
    "        \n",
    "        x  = x.unsqueeze(2).expand(batch_size, x.size(1), num_clusters, in_channels)\n",
    "        clusters = clusters.view(1,1,num_clusters,in_channels)\n",
    "        \n",
    "        norm = scale * (x - clusters).pow(2).sum(dim=3)\n",
    "        \n",
    "        return norm\n",
    "    def gather(self, theta, x , clusters)->torch.tensor:\n",
    "        \"\"\"gather up all the clusters together into a tensor\n",
    "        args\n",
    "        x : input features\n",
    "        theta : learned cluster weights\n",
    "        clusters :  learned clusters\n",
    "        \n",
    "        returns the accumlated tensor\"\"\"\n",
    "        \n",
    "        num_clusters, channels = clusters.size()\n",
    "        clusters = clusters.view((1,1,  num_clusters, channels))\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.unsqueeze(2).expand((batch_size, x.size(1), num_clusters, channels))\n",
    "        residual = x - clusters\n",
    "        encoded_feat = (theta.unsqueeze(3)*residual).sum(dim=1)\n",
    "        \n",
    "        return encoded_feat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"forward function for  surface encoding\n",
    "        Args\n",
    "        x: input features\n",
    "        \n",
    "        returns the residual features\"\"\"\n",
    "        assert x.dim() == 4  and x.size(1) == self.inchannels, x.shape\n",
    "        x = Rearrange(x , 'b c h w -> b (h w) c').contiguous()\n",
    "        theta = nn.softmax(self.l2_norm(x, self.clusters, self.scale), dim = 2)\n",
    "        #accumelate\n",
    "        encoded_feat = self.gather(theta, x, self.clusters)\n",
    "        \n",
    "        return encoded_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "\n",
    "class AliasMultinomial(nn.Module):\n",
    "    def __init___(self, prob):\n",
    "        super(AliasMultinomial, self).__init__()\n",
    "        assert isclose(prob.sum().item(), 1), 'nosie distrubtion 1'\n",
    "        cpu_prob = prob.cpu()\n",
    "        K = len(prob)\n",
    "        \n",
    "        self.prob = [0] * K\n",
    "        self.alias = [0] * K \n",
    "        \n",
    "        small = []\n",
    "        large = []\n",
    "        for index, prob in enumerate(cpu_prob):\n",
    "            self.prob[index] = K* prob\n",
    "            if self.prob[index] < 1.0:\n",
    "                small.append(index)\n",
    "            else: \n",
    "                large.append(index)\n",
    "        while len(small) > 0 and len(large) > 0: \n",
    "            small = small.pop(\n",
    "                \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "\n",
    "# 2. Do the same search as before\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=[-122.6, 37.6, -122.3, 37.9],\n",
    "    datetime=\"2021-08-01/2021-08-31\",\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "items = list(search.get_items())\n",
    "\n",
    "# 3. Display each item’s thumbnail\n",
    "for item in items:\n",
    "    thumb_href = item.assets[\"thumbnail\"].href  # usually a small JPEG/PNG\n",
    "    print(item.id, item.datetime)\n",
    "    display(Image(url=thumb_href))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
